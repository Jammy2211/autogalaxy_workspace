{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modeling: Start Here\n",
        "====================\n",
        "\n",
        "This script is the starting point for modeling of interferometer datasets with less than 1000\n",
        "visibilities (e.g. SMA, ALMA) and it provides an overview of the modeling API.\n",
        "\n",
        "__Number of Visibilities__\n",
        "\n",
        "This example fits a **low-resolution interferometric dataset** with a small number of visibilities (273). The\n",
        "dataset is intentionally minimal so that the example runs quickly and allows you to become familiar with the API\n",
        "and modeling workflow. The code demonstrated in this example can feasible fit datasets with up to around 10000\n",
        "visibilities, above which computational time and VRAM use become significant for this modeling approach.\n",
        "\n",
        "High-resolution datasets with many visibilities (e.g. high-quality ALMA observations\n",
        "with **millions hundreds of millions of visibilities**) can be modeled efficiently. However, this requires\n",
        "using the more advanced **pixelized source reconstructions** modeling approach. These large datasets fully\n",
        "exploit **JAX acceleration**, enable modeling to run in **hours on a modern GPU**.\n",
        "\n",
        "If your dataset contains many visibilities, you should start by working through this example and the other examples\n",
        "in the `interferometer` folder. Once you are comfortable with the API, the `feature/pixelization` package provides a\n",
        "guided path toward efficiently modeling large interferometric datasets.\n",
        "\n",
        "The threshold between a dataset having many visibilities and therefore requiring pixelized source reconstructions, or\n",
        "being small enough to be modeled with light profiles, is around **10,000 visibilities**.\n",
        "\n",
        "__Model__\n",
        "\n",
        "This script fits `Interferometer` dataset of a galaxy with a model where:\n",
        "\n",
        " - The galaxy's light is a linear parametric `Sersic` bulge and `Exponential` disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from pathlib import Path\n",
        "import autofit as af\n",
        "import autogalaxy as ag\n",
        "import autogalaxy.plot as aplt\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "We define the \u2018real_space_mask\u2019 which defines the grid the image the galaxy is evaluated using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask_radius = 4.0\n",
        "\n",
        "real_space_mask = ag.Mask2D.circular(\n",
        "    shape_native=(256, 256),\n",
        "    pixel_scales=0.1,\n",
        "    radius=mask_radius,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "Load and plot the galaxy `Interferometer` dataset `simple__sersic` from .fits files, which we will fit \n",
        "with the model.\n",
        "\n",
        "This includes the method used to Fourier transform the real-space image of the galaxy to the uv-plane and compare \n",
        "directly to the visiblities. We use a non-uniform fast Fourier transform, which is the most efficient method for \n",
        "interferometer datasets containing ~1-10 million visibilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple\"\n",
        "dataset_path = Path(\"dataset\") / \"interferometer\" / dataset_name\n",
        "\n",
        "dataset = ag.Interferometer.from_fits(\n",
        "    data_path=dataset_path / \"data.fits\",\n",
        "    noise_map_path=dataset_path / \"noise_map.fits\",\n",
        "    uv_wavelengths_path=dataset_path / \"uv_wavelengths.fits\",\n",
        "    real_space_mask=real_space_mask,\n",
        "    transformer_class=ag.TransformerDFT,\n",
        ")\n",
        "\n",
        "dataset_plotter = aplt.InterferometerPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()\n",
        "dataset_plotter.subplot_dirty_images()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Over Sampling__\n",
        "\n",
        "If you are familiar with using imaging data, you may have seen that a numerical technique called over sampling is used, \n",
        "which evaluates light profiles on a higher resolution grid than the image data to ensure the calculation is accurate.\n",
        "\n",
        "Interferometer does not observe galaxies in a way where over sampling is necessary, therefore all interferometer\n",
        "calculations are performed without over sampling.\n",
        "\n",
        "__Model__\n",
        "\n",
        "We compose our model using `Model` objects, which represent the galaxies we fit to our data. In this \n",
        "example we fit a model where:\n",
        "\n",
        " - The galaxy's light is a linear parametric `Sersic` bulge and `Exponential` disk, the centres of \n",
        " which are aligned [10 parameters].\n",
        " \n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=10.\n",
        "\n",
        "__Linear Light Profiles__\n",
        "\n",
        "The model below uses a `linear light profile` for the bulge and disk, via the API `lp_linear`. This is a specific type \n",
        "of light profile that solves for the `intensity` of each profile that best fits the data via a linear inversion. \n",
        "This means it is not a free parameter, reducing the dimensionality of non-linear parameter space. \n",
        "\n",
        "Linear light profiles significantly improve the speed, accuracy and reliability of modeling and they are used\n",
        "by default in every modeling example. A full description of linear light profiles is provided in the\n",
        "`autogalaxy_workspace/*/modeling/imaging/features/linear_light_profiles.py` example.\n",
        "\n",
        "A standard light profile can be used if you change the `lp_linear` to `lp`, but it is not recommended.\n",
        "\n",
        "__Coordinates__\n",
        "\n",
        "The model fitting default settings assume that the galaxy centre is near the coordinates (0.0\", 0.0\"). \n",
        "\n",
        "If for your dataset the galaxy is not centred at (0.0\", 0.0\"), we recommend that you either: \n",
        "\n",
        " - Reduce your data so that the centre is (`autogalaxy_workspace/*/preprocess`). \n",
        " - Manually override the model priors (`autogalaxy_workspace/*/modeling/imaging/customize/priors.py`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bulge = af.Model(ag.lp_linear.Sersic)\n",
        "disk = af.Model(ag.lp_linear.Exponential)\n",
        "bulge.centre = disk.centre\n",
        "\n",
        "galaxy = af.Model(ag.Galaxy, redshift=0.5, bulge=bulge, disk=disk)\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(galaxy=galaxy))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format.\n",
        "\n",
        "[The `info` below may not display optimally on your computer screen, for example the whitespace between parameter\n",
        "names on the left and parameter priors on the right may lead them to appear across multiple lines. This is a\n",
        "common issue in Jupyter notebooks.\n",
        "\n",
        "The`info_whitespace_length` parameter in the file `config/generag.yaml` in the [output] section can be changed to \n",
        "increase or decrease the amount of whitespace (The Jupyter notebook kernel will need to be reset for this change to \n",
        "appear in a notebook).]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "The model is fitted to the data using a non-linear search. In this example, we use the nested sampling algorithm \n",
        "Nautilus (https://nautilus.readthedocs.io/en/latest/).\n",
        "\n",
        "The folders: \n",
        "\n",
        " - `autogalaxy_workspace/*/modeling/imaging/searches`.\n",
        " - `autogalaxy_workspace/*/modeling/imaging/customize`\n",
        "  \n",
        "Give overviews of the  non-linear searches **PyAutoGalaxy** supports and more details on how to customize the\n",
        "model-fit, including the priors on the model. \n",
        "\n",
        "The `name` and `path_prefix` below specify the path where results are stored in the output folder:  \n",
        "\n",
        " `/autogalaxy_workspace/output/imaging/simple__sersic/mass[sie]/unique_identifier`.\n",
        "\n",
        "__Unique Identifier__\n",
        "\n",
        "In the path above, the `unique_identifier` appears as a collection of characters, where this identifier is generated \n",
        "based on the model, search and dataset that are used in the fit.\n",
        " \n",
        "An identical combination of model, search and dataset generates the same identifier, meaning that rerunning the\n",
        "script will use the existing results to resume the model-fit. In contrast, if you change the model, search or dataset,\n",
        "a new unique identifier will be generated, ensuring that the model-fit results are output into a separate folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=Path(\"interferometer\", \"modeling\"),\n",
        "    name=\"start_here\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        "    n_batch=50,  # GPU model fits are batched and run simultaneously, see VRAM section below.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "The `AnalysisInterferometer` object defines the `log_likelihood_function` used by the non-linear search to fit the \n",
        "model to the `Interferometer`dataset.\n",
        "\n",
        "__JAX__\n",
        "\n",
        "PyAutouses JAX under the hood for fast GPU/CPU acceleration. If JAX is installed with GPU\n",
        "support, your fits will run much faster (around 10 minutes instead of an hour). If only a CPU is available,\n",
        "JAX will still provide a speed up via multithreading, with fits taking around 20-30 minutes.\n",
        "\n",
        "If you don\u2019t have a GPU locally, consider Google Colab which provides free GPUs, so your modeling runs are much faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = ag.AnalysisInterferometer(dataset=dataset, use_jax=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__VRAM Use__\n",
        "\n",
        "When running Autowith JAX on a GPU, the analysis must fit within the GPU\u2019s available VRAM. If insufficient \n",
        "VRAM is available, the analysis will fail with an out-of-memory error, typically during JIT compilation or the \n",
        "first likelihood call.\n",
        "\n",
        "Two factors dictate the VRAM usage of an analysis:\n",
        "\n",
        "- The number of arrays and other data structures JAX must store in VRAM to fit the model\n",
        "  to the data in the likelihood function. This is dictated by the model complexity and dataset size.\n",
        "\n",
        "- The `batch_size` sets how many likelihood evaluations are performed simultaneously.\n",
        "  Increasing the batch size increases VRAM usage but can reduce overall run time,\n",
        "  while decreasing it lowers VRAM usage at the cost of slower execution.\n",
        "\n",
        "Before running an analysis, users should check that the estimated VRAM usage for the\n",
        "chosen batch size is comfortably below their GPU\u2019s total VRAM.\n",
        "\n",
        "The method below prints the VRAM usage estimate for the analysis and model with the specified batch size,\n",
        "it takes about 20-30 seconds to run so you may want to comment it out once you are familiar with your GPU's VRAM limits.\n",
        "\n",
        "For a MGE model with the low visibility dataset fitted in this example VRAM use is relatively low (~0.3GB) For other \n",
        "models (e.g. pixelized sources) and datasets with more visibilities it can be much higher (> 1GB going beyond 10GB)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis.print_vram_use(model=model, batch_size=search.batch_size)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Times__\n",
        "\n",
        "Modeling can be a computationally expensive process. When fitting complex models to high resolution datasets \n",
        "run times can be of order hours, days, weeks or even months.\n",
        "\n",
        "Run times are dictated by two factors:\n",
        "\n",
        " - The log likelihood evaluation time: the time it takes for a single `instance` of the model to be fitted to \n",
        "   the dataset such that a log likelihood is returned.\n",
        " \n",
        " - The number of iterations (e.g. log likelihood evaluations) performed by the non-linear search: more complex lens\n",
        "   models require more iterations to converge to a solution.\n",
        "   \n",
        "For this analysis, the log likelihood evaluation time is ~0.01 seconds on CPU, < 0.001 seconds on GPU, which is \n",
        "extremely fast for modeling. \n",
        "\n",
        "To estimate the expected overall run time of the model-fit we multiply the log likelihood evaluation time by an \n",
        "estimate of the number of iterations the non-linear search will perform. For this model, this is typically around\n",
        "? iterations, meaning that this script takes ? on CPU and ? on GPU.\n",
        "\n",
        "__Model-Fit__\n",
        "\n",
        "We can now begin the model-fit by passing the model and analysis object to the search, which performs the \n",
        "Nautilus non-linear search in order to find which models fit the data with the highest likelihood.\n",
        "\n",
        "**Run Time Error:** On certain operating systems (e.g. Windows, Linux) and Python versions, the code below may produce \n",
        "an error. If this occurs, see the `autolens_workspace/guides/modeling/bug_fix` example for a fix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output Folder__\n",
        "\n",
        "Now this is running you should checkout the `autogalaxy_workspace/output` folder. This is where the results of the \n",
        "search are written to hard-disk (in the `start_here` folder), where all outputs are human readable (e.g. as .json,\n",
        ".csv or text files).\n",
        "\n",
        "As the fit progresses, results are written to the `output` folder on the fly using the highest likelihood model found\n",
        "by the non-linear search so far. This means you can inspect the results of the model-fit as it runs, without having to\n",
        "wait for the non-linear search to terminate.\n",
        " \n",
        "The `output` folder includes:\n",
        "\n",
        " - `model.info`: Summarizes the model, its parameters and their priors discussed in the next tutorial.\n",
        " \n",
        " - `model.results`: Summarizes the highest likelihood model inferred so far including errors.\n",
        " \n",
        " - `images`: Visualization of the highest likelihood model-fit to the dataset, (e.g. a fit subplot showing the \n",
        " galaxies, model data and residuals).\n",
        " \n",
        " - `files`: A folder containing .fits files of the dataset, the model as a human-readable .json file, \n",
        " a `.csv` table of every non-linear search sample and other files containing information about the model-fit.\n",
        " \n",
        " - search.summary: A file providing summary statistics on the performance of the non-linear search.\n",
        " \n",
        " - `search_internal`: Internal files of the non-linear search (in this case Nautilus) used for resuming the fit and\n",
        "  visualizing the search.\n",
        "\n",
        "__Result__\n",
        "\n",
        "The search returns a result object, which whose `info` attribute shows the result in a readable format.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Result` object also contains:\n",
        "\n",
        " - The model corresponding to the maximum log likelihood solution in parameter space.\n",
        " - The corresponding maximum log likelihood `Galaxies` and `FitInterferometer` objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.max_log_likelihood_instance)\n",
        "\n",
        "galaxies_plotter = aplt.GalaxiesPlotter(\n",
        "    galaxies=result.max_log_likelihood_galaxies,\n",
        "    grid=real_space_mask.derive_grid.unmasked,\n",
        ")\n",
        "galaxies_plotter.subplot()\n",
        "fit_plotter = aplt.FitInterferometerPlotter(fit=result.max_log_likelihood_fit)\n",
        "fit_plotter.subplot_fit()\n",
        "fit_plotter.subplot_fit_dirty_images()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result contains the full posterior information of our non-linear search, including all parameter samples, \n",
        "log likelihood values and tools to compute the errors on the model. \n",
        "\n",
        "There are built in visualization tools for plotting this.\n",
        "\n",
        "The plot is labeled with short hand parameter names (e.g. `sersic_index` is mapped to the short hand \n",
        "parameter `n`). These mappings ate specified in the `config/notation.yaml` file and can be customized by users.\n",
        "\n",
        "The superscripts of labels correspond to the name each component was given in the model (e.g. for the `Isothermal`\n",
        "mass its name `mass` defined when making the `Model` above is used)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.NestPlotter(samples=result.samples)\n",
        "plotter.corner_cornerpy()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script gives a concise overview of the basic modeling API, fitting one of the simplest galaxy models possible.\n",
        "\n",
        "Let\u2019s now consider what features you should read about to improve your galaxy modeling, especially if you are aiming\n",
        "to fit more complex models to your data.\n",
        "\n",
        "__Features__\n",
        "\n",
        "The examples in the `autogalaxy_workspace/*/interferometer/features` package illustrate other galaxy modeling\n",
        "features.\n",
        "\n",
        "We recommend you check out just one feature next, because it makes galaxy modeling of interferometer datasets\n",
        "more reliable and efficient, and will then allow you to model high-resolution datasets with many visibilities:\n",
        "\n",
        "- ``pixelization``: The galaxy\u2019s light is reconstructed using an adaptive Rectangular mesh or Delaunay mesh.\n",
        "\n",
        "The files `autogalaxy_workspace/*/guides/modeling/searches` and\n",
        "`autogalaxy_workspace/*/guides/modeling/customize` provide guides on how to customize many other aspects of the\n",
        "model-fit. Check them out to see if anything sounds useful, but for most users you can get by without using these\n",
        "forms of customization!\n",
        "\n",
        "__Data Preparation__\n",
        "\n",
        "If you are looking to fit your own interferometer data of a galaxy, check out\n",
        "the `autogalaxy_workspace/*/interferometer/data_preparation/start_here.ipynb` script for an overview of how data\n",
        "should be prepared before being modeled.\n",
        "\n",
        "__HowToGalaxy__\n",
        "\n",
        "This `start_here.py` script, and the features examples above, do not explain many details of how galaxy modeling is\n",
        "performed, for example:\n",
        "\n",
        "- How does PyAutoGalaxy evaluate galaxy light profiles and perform image-plane calculations?\n",
        "- How is a galaxy model fitted to data? What quantifies the goodness of fit (e.g. how is a log likelihood computed)?\n",
        "- How does Nautilus find the highest likelihood galaxy models? What exactly is a \u201cnon-linear search\u201d?\n",
        "\n",
        "You do not need to be able to answer these questions in order to fit galaxy models with PyAutoGalaxy and do science.\n",
        "However, having a deeper understanding of how it all works is both interesting and will benefit you as a scientist.\n",
        "\n",
        "This deeper insight is offered by the **HowToGalaxy** Jupyter notebook lectures, found\n",
        "at `autogalaxy_workspace/*/howtogalaxy`.\n",
        "\n",
        "I recommend that you check them out if you are interested in more details!\n",
        "\n",
        "__Modeling Customization__\n",
        "\n",
        "The folders `autogalaxy_workspace/*/guides/modeling/searches` give an overview of alternative non-linear searches,\n",
        "other than Nautilus, that can be used to fit galaxy models.\n",
        "\n",
        "They also provide details on how to customize the model-fit, for example the priors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}