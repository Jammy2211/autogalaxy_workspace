{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modeling: Multi Modeling\n",
        "========================\n",
        "\n",
        "This script fits multiple multi-wavelength `Imaging` datasets of a galaxy with a model where:\n",
        "\n",
        " - The galaxy's light is a linear parametric `Sersic` bulge and `Exponential` disk.\n",
        "\n",
        "Two images are fitted, corresponding to a greener ('g' band) and redder image (`r` band).\n",
        "\n",
        "This is an advanced script and assumes previous knowledge of the core **PyAutoGalaxy** API for galaxy modeling. Thus,\n",
        "certain parts of code are not documented to ensure the script is concise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from pathlib import Path\n",
        "import autofit as af\n",
        "import autogalaxy as ag\n",
        "import autogalaxy.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Colors__\n",
        "\n",
        "The colors of the multi-wavelength image, which in this case are green (g-band) and red (r-band).\n",
        "\n",
        "The strings are used for load each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "color_list = [\"g\", \"r\"]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Pixel Scales__\n",
        "\n",
        "Every multi-wavelength dataset can have its own unique pixel-scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pixel_scales_list = [0.08, 0.12]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "Load and plot each multi-wavelength galaxy dataset, using a list of their waveband colors.\n",
        "\n",
        "Note how the disk appears brighter in the g-band image, whereas the bulge is clearer in the r-band image.\n",
        "Multi-wavelength image can therefore better decompose the structure of galaxies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_type = \"multi\"\n",
        "dataset_label = \"imaging\"\n",
        "dataset_name = \"simple\"\n",
        "\n",
        "dataset_path = Path(\"dataset\") / dataset_type / dataset_label / dataset_name\n",
        "\n",
        "dataset_list = [\n",
        "    ag.Imaging.from_fits(\n",
        "        data_path=Path(dataset_path) / f\"{color}_data.fits\",\n",
        "        psf_path=Path(dataset_path) / f\"{color}_psf.fits\",\n",
        "        noise_map_path=Path(dataset_path) / f\"{color}_noise_map.fits\",\n",
        "        pixel_scales=pixel_scales,\n",
        "    )\n",
        "    for color, pixel_scales in zip(color_list, pixel_scales_list)\n",
        "]\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "    dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "The model-fit requires a 2D mask defining the regions of the image we fit the galaxy model to the data, which we define\n",
        "and use to set up the `Imaging` object that the galaxy model fits.\n",
        "\n",
        "For multi-wavelength galaxy modeling, we use the same mask for every dataset whenever possible. This is not\n",
        "absolutely necessary, but provides a more reliable analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask_list = [\n",
        "    ag.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        "    )\n",
        "    for dataset in dataset_list\n",
        "]\n",
        "\n",
        "dataset_list = [\n",
        "    dataset.apply_mask(mask=mask) for imaging, mask in zip(dataset_list, mask_list)\n",
        "]\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "    dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Over Sampling__\n",
        "\n",
        "Over sampling is a numerical technique where the images of light profiles and galaxies are evaluated \n",
        "on a higher resolution grid than the image data to ensure the calculation is accurate. \n",
        "\n",
        "For a new user, the details of over-sampling are not important, therefore just be aware that below we make it so that \n",
        "all calculations use an adaptive over sampling scheme which ensures high accuracy and precision.\n",
        "\n",
        "Once you are more experienced, you should read up on over-sampling in more detail via \n",
        "the `autogalaxy_workspace/*/guides/over_sampling.ipynb` notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset in dataset_list:\n",
        "    over_sample_size = ag.util.over_sample.over_sample_size_via_radial_bins_from(\n",
        "        grid=dataset.grid,\n",
        "        sub_size_list=[8, 4, 1],\n",
        "        radial_list=[0.3, 0.6],\n",
        "        centre_list=[(0.0, 0.0)],\n",
        "    )\n",
        "\n",
        "    dataset = dataset.apply_over_sampling(over_sample_size_lp=over_sample_size)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We compose our galaxy model using `Model` objects, which represent the galaxies we fit to our data. In this \n",
        "example we fit a galaxy model where:\n",
        "\n",
        " - The galaxy's bulge is a linear parametric `Sersic` bulge [6 parameters]. \n",
        "\n",
        " - The galaxy's disk is a linear parametric `Exponential` disk [6 parameters].\n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=15.\n",
        "\n",
        "__Model Extension__\n",
        "\n",
        "Galaxies change appearance across wavelength, for example their size.\n",
        "\n",
        "Models applied to combined analyses can be extended to include free parameters specific to each dataset. In this example,\n",
        "we want the galaxy's effective radii to vary across the g and r-band datasets, which will be illustrated below.\n",
        "\n",
        "__Linear Light Profiles__\n",
        "\n",
        "As an advanced user you should be familiar wiht linear light profiles, see elsewhere in the workspace for informaiton\n",
        "if not.\n",
        "\n",
        "For multi wavelength dataset modeling, the `lp_linear` API is extremely powerful as the `intensity` varies across\n",
        "the datasets, meaning that making it linear reduces the dimensionality of parameter space significantly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bulge = af.Model(ag.lp_linear.Sersic)\n",
        "disk = af.Model(ag.lp_linear.Exponential)\n",
        "\n",
        "galaxy = af.Model(ag.Galaxy, redshift=0.5, bulge=bulge, disk=disk)\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(galaxy=galaxy))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis List__\n",
        "\n",
        "Set up two instances of the `Analysis` class object, one for each dataset.\n",
        "\n",
        "__JAX__\n",
        "\n",
        "PyAutoLens uses JAX under the hood for fast GPU/CPU acceleration. If JAX is installed with GPU\n",
        "support, your fits will run much faster (around 10 minutes instead of an hour). If only a CPU is available,\n",
        "JAX will still provide a speed up via multithreading, with fits taking around 20-30 minutes.\n",
        "\n",
        "If you don\u2019t have a GPU locally, consider Google Colab which provides free GPUs, so your modeling runs are much faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = [\n",
        "    ag.AnalysisImaging(dataset=dataset, use_jax=True) for dataset in dataset_list\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factor__\n",
        "\n",
        "Each analysis object is wrapped in an `AnalysisFactor`, which pairs it with the model and prepares it for use in a \n",
        "factor graph. This step allows us to flexibly define how each dataset relates to the model.\n",
        "\n",
        "The term \"Factor\" comes from factor graphs, a type of probabilistic graphical model. In this context, each factor \n",
        "represents the connection between one dataset and the shared model.\n",
        "\n",
        "The API for extending the model across datasets is shown below, by overwriting the `effective_radius`\n",
        "variables of the model passed to each `AnalysisFactor` object with new priors, making each dataset have its own\n",
        "`effective_radius` free parameter.\n",
        "\n",
        "NOTE: Other aspects of galaxies may vary across wavelength, none of which are included in this example. The API below \n",
        "can easily be extended to include these additional parameters, and the `features` package explains other tools for \n",
        "extending the model across datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_factor_list = []\n",
        "\n",
        "for analysis in analysis_list:\n",
        "    model_analysis = model.copy()\n",
        "    model_analysis.galaxies.galaxy.bulge.effective_radius = af.UniformPrior(\n",
        "        lower_limit=0.0, upper_limit=10.0\n",
        "    )\n",
        "\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model_analysis, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Factor Graph__\n",
        "\n",
        "All `AnalysisFactor` objects are combined into a `FactorGraphModel`, which represents a global model fit to \n",
        "multiple datasets using a graphical model structure.\n",
        "\n",
        "The key outcomes of this setup are:\n",
        "\n",
        " - The individual log likelihoods from each `Analysis` object are summed to form the total log likelihood \n",
        "   evaluated during the model-fitting process.\n",
        "\n",
        " - Results from all datasets are output to a unified directory, with subdirectories for visualizations \n",
        "   from each analysis object, as defined by their `visualize` methods.\n",
        "\n",
        "This is a basic use of **PyAutoFit**'s graphical modeling capabilities, which support advanced hierarchical \n",
        "and probabilistic modeling for large, multi-dataset analyses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To inspect this new model, with extra parameters for each dataset created, we \n",
        "print `factor_graph.global_prior_model.info`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)\n",
        "\n",
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=Path(\"multi\") / \"modeling\",\n",
        "    name=\"start_here\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model-Fit__\n",
        "\n",
        "To fit multiple datasets, we pass the `FactorGraphModel` to a non-linear search.\n",
        "\n",
        "Unlike single-dataset fitting, we now pass the `factor_graph.global_prior_model` as the model and \n",
        "the `factor_graph` itself as the analysis object.\n",
        "\n",
        "This structure enables simultaneous fitting of multiple datasets in a consistent and scalable way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_list = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result object returned by this model-fit is a list of `Result` objects, because we used a factor graph.\n",
        "Each result corresponds to each analysis, and therefore corresponds to the model-fit at that wavelength.\n",
        "\n",
        "For example, close inspection of the `max_log_likelihood_instance` of the two results shows that all parameters,\n",
        "except the `effective_radius` of the source galaxy's `bulge`, are identical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_list[0].max_log_likelihood_instance)\n",
        "print(result_list[1].max_log_likelihood_instance)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting each result's galaxies shows that the galaxy appears different, owning to its different intensities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for result in result_list:\n",
        "    galaxies_plotter = aplt.GalaxiesPlotter(\n",
        "        galaxies=result.max_log_likelihood_galaxies, grid=result.grids.lp\n",
        "    )\n",
        "    galaxies_plotter.subplot_galaxies()\n",
        "\n",
        "    fit_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "    fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Samples` object still has the dimensions of the overall non-linear search (in this case N=16). \n",
        "\n",
        "Therefore, the samples is identical in every result object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.NestPlotter(samples=result_list.samples)\n",
        "plotter.corner_cornerpy()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "This simple example introduces the API for fitting multiple datasets with a shared model.\n",
        "\n",
        "It should already be quite intuitive how this API can be adapted to fit more complex models, or fit different\n",
        "datasets with different models. For example, an `AnalysisImaging` and `AnalysisInterferometer` can be combined, into\n",
        "a single factor graph model, to simultaneously fit a imaging and interferometric data.\n",
        "\n",
        "The `advanced/multi/modeling` package has more examples of how to fit multiple datasets with different models,\n",
        "including relational models that vary parameters across datasets as a function of wavelength."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}