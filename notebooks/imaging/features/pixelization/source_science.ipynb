{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pixelization: Source Reconstruction\n",
        "===================================\n",
        "\n",
        "A common pixelization use-case is to reconstruct the galaxy's surface brightness on a pixelization mesh, and\n",
        "then export this reconstruction to perform scientific analysis.\n",
        "\n",
        "It is beneficial to export this reconstruction in a format which is independent of modeling, so study of\n",
        "the source can be performed separately.\n",
        "\n",
        "This script illustrates how modeling outputs source reconstructions to a .csv file, and how this can be easily\n",
        "loaded to perform analysis without the need for PyAutoLens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from autoconf import jax_wrapper  # Sets JAX environment before other imports\n",
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import autofit as af\n",
        "import autogalaxy as ag\n",
        "import autogalaxy.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "The code below is identical to the pixelizaiton `modeling` example, crucially creating a model-fit which\n",
        "outputs the pixelization source reconstruction to a .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple__sersic\"\n",
        "dataset_path = Path(\"dataset\") / \"imaging\" / dataset_name\n",
        "\n",
        "dataset = ag.Imaging.from_fits(\n",
        "    data_path=dataset_path / \"data.fits\",\n",
        "    psf_path=dataset_path / \"psf.fits\",\n",
        "    noise_map_path=dataset_path / \"noise_map.fits\",\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "mask_radius = 3.0\n",
        "\n",
        "mask = ag.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native,\n",
        "    pixel_scales=dataset.pixel_scales,\n",
        "    radius=mask_radius,\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "dataset = dataset.apply_over_sampling(\n",
        "    over_sample_size_pixelization=4,\n",
        ")\n",
        "\n",
        "mesh_pixels_yx = 28\n",
        "mesh_shape = (mesh_pixels_yx, mesh_pixels_yx)\n",
        "\n",
        "pixelization = af.Model(\n",
        "    ag.Pixelization,\n",
        "    mesh=ag.mesh.RectangularAdaptDensity(shape=mesh_shape),\n",
        "    regularization=ag.reg.MaternKernel,\n",
        ")\n",
        "\n",
        "galaxy = af.Model(ag.Galaxy, redshift=0.5, pixelization=pixelization)\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(galaxy=galaxy))\n",
        "\n",
        "search = af.Nautilus(\n",
        "    path_prefix=Path(\"features\"),\n",
        "    name=\"pixelization\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        "    n_batch=20,\n",
        "    iterations_per_quick_update=50000,\n",
        ")\n",
        "\n",
        "analysis = ag.AnalysisImaging(dataset=dataset)\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Reconstruction CSV__\n",
        "\n",
        "In the results `image` folder there is a .csv file called `source_plane_reconstruction_0.csv` which contains the\n",
        "y and x coordinates of the pixelization mesh, the reconstruct values and the noise map of these values.\n",
        "\n",
        "This file is provides all information on the source reconstruction in a format that does not depend autolens\n",
        "and therefore be easily loaded to create images of the source or shared collaborations who do not have PyAutoLens\n",
        "installed.\n",
        "\n",
        "First, lets load `source_plane_reconstruction_0.csv` as a dictionary, using basic `csv` functionality in Python.\n",
        "\n",
        "NOTE: If the .csv file does not exist, we create a dictionary with the same format but with dummy values so the rest of\n",
        "the script can be run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import csv\n",
        "\n",
        "try:\n",
        "\n",
        "    with open(\n",
        "        search.paths.image_path / \"source_plane_reconstruction_0.csv\", mode=\"r\"\n",
        "    ) as file:\n",
        "        reader = csv.reader(file)\n",
        "        header_list = next(reader)  # ['y', 'x', 'reconstruction', 'noise_map']\n",
        "\n",
        "        reconstruction_dict = {header: [] for header in header_list}\n",
        "\n",
        "        for row in reader:\n",
        "            for key, value in zip(header_list, row):\n",
        "                reconstruction_dict[key].append(float(value))\n",
        "\n",
        "        # Convert lists to NumPy arrays\n",
        "        for key in reconstruction_dict:\n",
        "            reconstruction_dict[key] = np.array(reconstruction_dict[key])\n",
        "\n",
        "except FileNotFoundError:\n",
        "\n",
        "    print(\"`source_plane_reconstruction_0.csv` not found. Using dummy data instead.\")\n",
        "\n",
        "    x = np.array([-1.0, 0.0, 1.0, -1.0, 0.0, 1.0, -1.0, 0.0, 1.0])\n",
        "    y = np.array([1.0, 1.0, 1.0, 0.0, 0.0, 0.0, -1.0, -1.0, -1.0])\n",
        "    reconstruction = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0])\n",
        "    noise_map = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
        "\n",
        "    reconstruction_dict = {\n",
        "        \"x\": x,\n",
        "        \"y\": y,\n",
        "        \"reconstruction\": reconstruction,\n",
        "        \"noise_map\": noise_map,\n",
        "    }\n",
        "\n",
        "print(reconstruction_dict[\"y\"])\n",
        "print(reconstruction_dict[\"x\"])\n",
        "print(reconstruction_dict[\"reconstruction\"])\n",
        "print(reconstruction_dict[\"noise_map\"])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now use standard libraries to performed calculations with the reconstruction on the mesh, again avoiding\n",
        "the need to use autolens.\n",
        "\n",
        "For example, we can create a RectangularAdaptDensity mesh using the scipy.spatial library, which is a triangulation\n",
        "of the y and x coordinates of the pixelization mesh. This is useful for visualizing the pixelization\n",
        "and performing calculations on the mesh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import scipy\n",
        "\n",
        "points = np.stack(arrays=(reconstruction_dict[\"x\"], reconstruction_dict[\"y\"]), axis=-1)\n",
        "\n",
        "mesh = scipy.spatial.Delaunay(points)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpolating the result to a uniform grid is also possible using the scipy.interpolate library, which means the result\n",
        "can be turned into a uniform 2D image which can be useful to analyse the source with tools which require an uniform grid.\n",
        "\n",
        "Below, we interpolate the result onto a 201 x 201 grid of pixels with the extent spanning -1.0\" to 1.0\", which\n",
        "capture the majority of the source reconstruction without being too high resolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.interpolate import griddata\n",
        "\n",
        "values = reconstruction_dict[\"reconstruction\"]\n",
        "\n",
        "interpolation_grid = ag.Grid2D.from_extent(\n",
        "    extent=(-1.0, 1.0, -1.0, 1.0), shape_native=(201, 201)\n",
        ")\n",
        "\n",
        "interpolated_array = griddata(points=points, values=values, xi=interpolation_grid)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}