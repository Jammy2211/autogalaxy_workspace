{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 3: Graphical Benefits\n",
        "==============================\n",
        "\n",
        "In the previous tutorials, we fitted a dataset containing 3 galaxies which had a shared `sersic_index` value.\n",
        "\n",
        "We used different approaches to estimate the shared `sersic_index`, for example a simple approach of fitting each\n",
        "dataset one-by-one and estimating the Sersic index via a weighted average or posterior multiplication and a more\n",
        "complicated approach using a graphical model.\n",
        "\n",
        "The estimates were consistent with one another, making it hard to justify the use of the more complicated graphical\n",
        "model. However, the model fitted in the previous tutorial was extremely simple, and by making it slightly more complex\n",
        "in this tutorial we will be able to show the benefits of using the graphical modeling approach.\n",
        "\n",
        "__Sample Simulation__\n",
        "\n",
        "The dataset fitted in this example script is simulated imaging data of a sample of 3 galaxies.\n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the\n",
        "script `autogalaxy_workspace/scripts/simulators/imaging/samples/dev_exp.py`.\n",
        "\n",
        "__The Model__\n",
        "\n",
        "The more complex datasets and model fitted in this tutorial is an extension of those fitted in the previous tutorial.\n",
        "\n",
        "Previously, there was only a bulge in each galaxy dataset which all had the same Sersic index.\n",
        "\n",
        "In this tutorial, each dataset now contains a bulge and disk, where all bulges have `sersic_index=4` and all disks\n",
        "`sersic_index=1.0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from os import path\n",
        "\n",
        "import autofit as af\n",
        "import autogalaxy as ag\n",
        "import autogalaxy.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "For each galaxy dataset in our sample we set up the correct path and load it by iterating over a for loop. \n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the \n",
        "script `autogalaxy_workspace/scripts/simulators/imaging/samples/dev_exp.py`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_label = \"samples\"\n",
        "dataset_type = \"imaging\"\n",
        "dataset_sample_name = \"dev_exp\"\n",
        "\n",
        "dataset_path = path.join(\"dataset\", dataset_type, dataset_label, dataset_sample_name)\n",
        "\n",
        "total_datasets = 3\n",
        "\n",
        "dataset_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_sample_path = path.join(dataset_path, f\"dataset_{dataset_index}\")\n",
        "\n",
        "    dataset_list.append(\n",
        "        ag.Imaging.from_fits(\n",
        "            data_path=path.join(dataset_sample_path, \"data.fits\"),\n",
        "            psf_path=path.join(dataset_sample_path, \"psf.fits\"),\n",
        "            noise_map_path=path.join(dataset_sample_path, \"noise_map.fits\"),\n",
        "            pixel_scales=0.1,\n",
        "        )\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "We now mask each galaxy in our dataset, using the imaging list we created above.\n",
        "\n",
        "We will assume a 3.0\" mask for every galaxy in the dataset is appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "masked_imaging_list = []\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    mask = ag.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        "    )\n",
        "\n",
        "    masked_imaging_list.append(dataset.apply_mask(mask=mask))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Paths__\n",
        "\n",
        "The path the results of all model-fits are output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path_prefix = path.join(\"imaging\", \"graphical\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model (one-by-one)__\n",
        "\n",
        "We are first going to fit each dataset one by one.\n",
        "\n",
        "We therefore fit a model where\n",
        "\n",
        " - The galaxy's bulge is a parametric `Sersic` bulge with its centre fixed to the input \n",
        " value of (0.0, 0.0) [5 parameters]. \n",
        "\n",
        " - The galaxy's disk is a parametric `Sersic` disk with its centre fixed to the input \n",
        " value of (0.0, 0.0) [5 parameters]. \n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=10.\n",
        "\n",
        "We require that the bulge Sersic index is between 3.0 and 6.0 and disk Sersic index 0.5 to 3.0 -- this ensures that\n",
        "the model does not swap the two components and fit the bulge with the lower Sersic index component and visa versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bulge = af.Model(ag.lp.Sersic)\n",
        "bulge.centre = (0.0, 0.0)\n",
        "bulge.sersic_index = af.UniformPrior(lower_limit=3.0, upper_limit=6.0)\n",
        "\n",
        "disk = af.Model(ag.lp.Sersic)\n",
        "disk.centre = (0.0, 0.0)\n",
        "disk.sersic_index = af.UniformPrior(lower_limit=0.5, upper_limit=3.0)\n",
        "\n",
        "galaxy = af.Model(ag.Galaxy, redshift=0.5, bulge=bulge, disk=disk)\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(galaxy=galaxy))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "For each dataset we now create a corresponding `Analysis` class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for dataset_index, masked_dataset in enumerate(masked_imaging_list):\n",
        "    #\n",
        "    analysis = ag.AnalysisImaging(dataset=masked_dataset)\n",
        "\n",
        "    analysis_list.append(analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fits (one-by-one)__\n",
        "\n",
        "For each dataset we now create a non-linear search, analysis and perform the model-fit using this model.\n",
        "\n",
        "The `Result` is stored in the list `result_list` and they are output to a unique folder named using the `dataset_index`.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_list = []\n",
        "\n",
        "for dataset_index, analysis in enumerate(analysis_list):\n",
        "    dataset_name_with_index = f\"dataset_{dataset_index}\"\n",
        "    path_prefix_with_index = path.join(path_prefix, \"tutorial_3_graphical_benefits\")\n",
        "\n",
        "    search = af.Nautilus(\n",
        "        path_prefix=path_prefix_with_index, name=dataset_name_with_index, n_live=100\n",
        "    )\n",
        "\n",
        "    result = search.fit(model=model, analysis=analysis)\n",
        "    result_list.append(result)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Sersic Index Estimates (Weighted Average)__\n",
        "\n",
        "We can now compute the Sersic index estimate of both light profiles, including their errors, from the individual \n",
        "model fits performed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_list = [result.samples for result in result_list]\n",
        "\n",
        "mp_instances = [samps.median_pdf() for samps in samples_list]\n",
        "mp_bulge_sersic_indexes = [\n",
        "    instance.galaxies.galaxy.bulge.sersic_index for instance in mp_instances\n",
        "]\n",
        "\n",
        "ue1_instances = [samp.values_at_upper_sigma(sigma=1.0) for samp in samples_list]\n",
        "le1_instances = [samp.values_at_lower_sigma(sigma=1.0) for samp in samples_list]\n",
        "\n",
        "ue1_bulge_sersic_indexes = [\n",
        "    instance.galaxies.galaxy.bulge.sersic_index for instance in ue1_instances\n",
        "]\n",
        "le1_bulge_sersic_indexes = [\n",
        "    instance.galaxies.galaxy.bulge.sersic_index for instance in le1_instances\n",
        "]\n",
        "\n",
        "error_list = [\n",
        "    ue1 - le1 for ue1, le1 in zip(ue1_bulge_sersic_indexes, le1_bulge_sersic_indexes)\n",
        "]\n",
        "\n",
        "values = np.asarray(mp_bulge_sersic_indexes)\n",
        "sigmas = np.asarray(error_list)\n",
        "\n",
        "weights = 1 / sigmas**2.0\n",
        "weight_averaged = np.sum(1.0 / sigmas**2)\n",
        "\n",
        "bulge_weighted_sersic_index = np.sum(values * weights) / np.sum(weights, axis=0)\n",
        "bulge_weighted_error = 1.0 / np.sqrt(weight_averaged)\n",
        "\n",
        "\n",
        "mp_instances = [samps.median_pdf() for samps in samples_list]\n",
        "mp_disk_sersic_indexes = [\n",
        "    instance.galaxies.galaxy.disk.sersic_index for instance in mp_instances\n",
        "]\n",
        "\n",
        "ue1_instances = [samp.values_at_upper_sigma(sigma=1.0) for samp in samples_list]\n",
        "le1_instances = [samp.values_at_lower_sigma(sigma=1.0) for samp in samples_list]\n",
        "\n",
        "ue1_disk_sersic_indexes = [\n",
        "    instance.galaxies.galaxy.disk.sersic_index for instance in ue1_instances\n",
        "]\n",
        "le1_disk_sersic_indexes = [\n",
        "    instance.galaxies.galaxy.disk.sersic_index for instance in le1_instances\n",
        "]\n",
        "\n",
        "error_list = [\n",
        "    ue1 - le1 for ue1, le1 in zip(ue1_disk_sersic_indexes, le1_disk_sersic_indexes)\n",
        "]\n",
        "\n",
        "values = np.asarray(mp_disk_sersic_indexes)\n",
        "sigmas = np.asarray(error_list)\n",
        "\n",
        "weights = 1 / sigmas**2.0\n",
        "weight_averaged = np.sum(1.0 / sigmas**2)\n",
        "\n",
        "disk_weighted_sersic_index = np.sum(values * weights) / np.sum(weights, axis=0)\n",
        "disk_weighted_error = 1.0 / np.sqrt(weight_averaged)\n",
        "\n",
        "\n",
        "print(\n",
        "    f\"Weighted Average Bulge Sersic Index Estimate = {bulge_weighted_sersic_index} ({bulge_weighted_error}) [1.0 sigma confidence intervals]\"\n",
        ")\n",
        "print(\n",
        "    f\"Weighted Average Disk Sersic Index Estimate = {disk_weighted_sersic_index} ({disk_weighted_error}) [1.0 sigma confidence intervals]\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The estimate of the Sersic indexes are not accurate, with both estimates well offset from the input values \n",
        "of 4.0 and 1.0.\n",
        "\n",
        "We will next show that the graphical model offers a notable improvement, but first lets consider why this\n",
        "approach is suboptimag.\n",
        "\n",
        "The most important difference between this model and the model fitted in the previous tutorial is that there are now\n",
        "two shared parameters we are trying to estimate, *and they are degenerate with one another*.\n",
        "\n",
        "We can see this by inspecting the probability distribution function (PDF) of the fit, placing particular focus on the \n",
        "2D degeneracy between the Sersic index of the bulge and disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_plotter = aplt.NautilusPlotter(samples=result_list[0].samples)\n",
        "search_plotter.cornerplot()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The problem is that the simple approach of taking a weighted average does not capture the curved banana-like shape\n",
        "of the PDF between the two Sersic indexes. This leads to significant error over estimation and biased inferences on the \n",
        "estimates.\n",
        "\n",
        "__Discussion__\n",
        "\n",
        "Let us now consider other downsides of fitting each dataset one-by-one, from a more statistical perspective. We \n",
        "will contrast these to the graphical model later in the tutorial.\n",
        "\n",
        "1) By fitting each dataset one-by-one this means that each model-fit fails to fully exploit the information we know \n",
        "about the global model. We *know* that there are only two single shared values of `sersic_index` across the full dataset \n",
        "that we want to estimate. However, each individual fit has its own `sersic_index` value which is able to assume \n",
        "different values than the `sersic_index` values used to fit the other datasets. This means that the large degeneracies \n",
        "between the two Sersic indexes emerge for each model-fit.\n",
        "\n",
        "By not fitting our model as a global model, we do not maximize the amount of information that we can extract from the \n",
        "dataset as a whole. If a model fits dataset 1 particularly bad, this *should* be reflected in how we interpret how \n",
        "well the model fits datasets 2 and 3. Our non-linear search should have a global view of how well the model fits the \n",
        "whole dataset. This is the *crucial aspect of fitting each dataset individually that we miss*, and what a graphical \n",
        "model addresses.\n",
        "\n",
        "2) When we combined the result to estimate the global `sersic_index` value via a weighted average, we marginalized over \n",
        "the samples in 1D. As showed above, when there are strong degeneracies between models parameters the information on \n",
        "the covariance between these parameters is lost when computing the global `sersic_index`. This increases the inferred \n",
        "uncertainties. A graphical model performs no such 1D marginalization and therefore fully samples the\n",
        "parameter covariances.\n",
        "\n",
        "3) In Bayesian inference it is important that we define priors on all of the model parameters. By estimating the \n",
        "global `sersic_index` after the model-fits are completed it is unclear what prior the global `sersic_index` a\n",
        "ctually has! We actually defined the prior five times -- once for each fit -- which is not a well defined prior.\n",
        "\n",
        "In a graphical model the prior is clearly defined.\n",
        "\n",
        "What would have happened if we had estimate the shared Sersic indexes via 2D posterior multiplication using a KDE? We\n",
        "will discuss this at the end of the tutorial after fitting a graphical model.\n",
        "\n",
        "__Model (Graphical)__\n",
        "\n",
        "We now compose a graphical model and fit it.\n",
        "\n",
        "Our model now consists of a galaxy with a bulge and disk, which each have a `sersic_index_shared_prior` variable, \n",
        "such that the same `sersic_index` parameters are used for the bulge and disks of all galaxies fitted to all datasets. \n",
        "\n",
        "We require that the bulge Sersic index is between 3.0 and 6.0 and disk Sersic index 0.5 to 3.0 -- this ensures that\n",
        "the model does not swap the two components and fit the bulge with the lower Sersic index component and visa versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bulge_sersic_index_shared_prior = af.UniformPrior(lower_limit=3.0, upper_limit=6.0)\n",
        "disk_sersic_index_shared_prior = af.UniformPrior(lower_limit=0.5, upper_limit=3.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now set up a list of `Model`'s, each of which contain a bulge and disk.\n",
        "\n",
        "All of these `Model`'s use the `sersic_index_shared_prior`'s above. This means all model-components use the same value \n",
        "of `sersic_index` for the bulge and same `sersic_index` values for the disk.\n",
        "\n",
        "For a fit to three datasets (each using an `Sersic` bulge and disk), this produces a parameter space with\n",
        "dimnensionality N=20 (8 parameters per pair of `Sersic` and 2 shared Sersic indexes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_list = []\n",
        "\n",
        "for model_index in range(total_datasets):\n",
        "    bulge = af.Model(ag.lp.Sersic)\n",
        "    bulge.centre = (0.0, 0.0)\n",
        "\n",
        "    # This makes every Galaxy bulge share the same `sersic_index`.\n",
        "    bulge.sersic_index = bulge_sersic_index_shared_prior\n",
        "\n",
        "    disk = af.Model(ag.lp.Sersic)\n",
        "    disk.centre = (0.0, 0.0)\n",
        "\n",
        "    # This makes every Galaxy disk share the same `sersic_index`.\n",
        "    disk.sersic_index = disk_sersic_index_shared_prior\n",
        "\n",
        "    galaxy = af.Model(ag.Galaxy, redshift=0.5, bulge=bulge, disk=disk)\n",
        "\n",
        "    model = af.Collection(galaxies=af.Collection(galaxy=galaxy))\n",
        "\n",
        "    model_list.append(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factors__\n",
        "\n",
        "We again create the graphical model using `AnalysisFactor` objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_factor_list = []\n",
        "\n",
        "for model, analysis in zip(model_list, analysis_list):\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Factor Graph__\n",
        "\n",
        "The analysis factors are then used to create the factor graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The factor graph model can again be printed via the `info` attribute, which shows that there are two shared\n",
        "parameters across the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "We can now create a non-linear search and use it to the fit the factor graph, again using its `global_prior_model` \n",
        "property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=path_prefix,\n",
        "    name=\"tutorial_3_graphical_benefits\",\n",
        "    n_live=250,\n",
        ")\n",
        "\n",
        "result = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result's `info` attribute shows that the result is expressed following the same structure of analysis factors\n",
        "that the `global_prior_model.info` attribute revealed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now inspect the inferred `sersic_index` values and compare this to the values estimated above via a weighted \n",
        "average.  \n",
        "\n",
        "(The errors of the weighted average is what was estimated for a run on my PC, yours may be slightly different!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bulge_sersic_index = result.samples.median_pdf()[0].galaxies.galaxy.bulge.sersic_index\n",
        "\n",
        "u1_error_0 = result.samples.values_at_upper_sigma(sigma=1.0)[\n",
        "    0\n",
        "].galaxies.galaxy.bulge.sersic_index\n",
        "l1_error_0 = result.samples.values_at_lower_sigma(sigma=1.0)[\n",
        "    0\n",
        "].galaxies.galaxy.bulge.sersic_index\n",
        "\n",
        "u3_error_0 = result.samples.values_at_upper_sigma(sigma=3.0)[\n",
        "    0\n",
        "].galaxies.galaxy.bulge.sersic_index\n",
        "l3_error_0 = result.samples.values_at_lower_sigma(sigma=3.0)[\n",
        "    0\n",
        "].galaxies.galaxy.bulge.sersic_index\n",
        "\n",
        "disk_sersic_index = result.samples.median_pdf()[0].galaxies.galaxy.disk.sersic_index\n",
        "\n",
        "u1_error_1 = result.samples.values_at_upper_sigma(sigma=1.0)[\n",
        "    0\n",
        "].galaxies.galaxy.disk.sersic_index\n",
        "l1_error_1 = result.samples.values_at_lower_sigma(sigma=1.0)[\n",
        "    0\n",
        "].galaxies.galaxy.disk.sersic_index\n",
        "\n",
        "u3_error_1 = result.samples.values_at_upper_sigma(sigma=3.0)[\n",
        "    0\n",
        "].galaxies.galaxy.disk.sersic_index\n",
        "l3_error_1 = result.samples.values_at_lower_sigma(sigma=3.0)[\n",
        "    0\n",
        "].galaxies.galaxy.disk.sersic_index\n",
        "\n",
        "print(\n",
        "    f\"Weighted Average Bulge Sersic Index Estimate = 3.035967168057999 (0.020862051618561108) [1.0 sigma confidence intervals]\\n\"\n",
        ")\n",
        "print(\n",
        "    f\"Weighted Average Disk Sersic Index Estimate = 1.0034699385233146 (0.011400000233187503) [1.0 sigma confidence intervals]\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Inferred value of the bulge Sersic index via a graphical fit to {total_datasets} datasets: \\n\"\n",
        ")\n",
        "print(\n",
        "    f\"{bulge_sersic_index} ({l1_error_0} {u1_error_0}) ({u1_error_0 - l1_error_0}) [1.0 sigma confidence intervals]\"\n",
        ")\n",
        "print(\n",
        "    f\"{bulge_sersic_index} ({l3_error_0} {u3_error_0}) ({u3_error_0 - l3_error_0}) [3.0 sigma confidence intervals]\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Inferred value of the disk Sersic index via a graphical fit to {total_datasets} datasets: \\n\"\n",
        ")\n",
        "print(\n",
        "    f\"{disk_sersic_index} ({l1_error_1} {u1_error_1}) ({u1_error_1 - l1_error_1}) [1.0 sigma confidence intervals]\"\n",
        ")\n",
        "print(\n",
        "    f\"{disk_sersic_index} ({l3_error_1} {u3_error_1}) ({u3_error_1 - l3_error_1}) [3.0 sigma confidence intervals]\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, using a graphical model allows us to infer a more precise and accurate model.\n",
        "\n",
        "You may already have an idea of why this is, but lets go over it in detail:\n",
        "\n",
        "__Discussion__\n",
        "\n",
        "Unlike a fit to each dataset one-by-one, the graphical model:\n",
        "\n",
        "1) Infers a PDF on the global Sersic index that fully accounts for the degeneracies between the models fitted to \n",
        "different datasets. This reduces significantly the large 2D degeneracies between the two Sersic indexes we saw when \n",
        "inspecting the PDFs of each individual fit.\n",
        "\n",
        "2) Fully exploits the information we know about the global model, for example that the Sersic index of every galaxy \n",
        "in every dataset is aligned. Now, the fit of the galaxy in dataset 1 informs the fits in datasets 2 and 3, and visa \n",
        "versa.\n",
        "\n",
        "3) Has a well defined prior on the global Sersic index, instead of independent priors on the Sersic index of each \n",
        "dataset.\n",
        "\n",
        "__Posterior Multiplication__\n",
        "\n",
        "What if we had combined the results of the individual model fits using 2D posterior multiplication via a KDE?\n",
        "\n",
        "This would produce an inaccurate estimate of the error, because each posterior contains the prior on the Sersic index \n",
        "multiple times which given the properties of this model should not be repeated.\n",
        "\n",
        "However, it is possible to convert each posterior to a likelihood (by dividing by its prior), combining these\n",
        "likelihoods to form a joint likelihood via 2D KDE multiplication and then insert just one prior back (agian using a 2D\n",
        "KDE) at the end to get a posterior which does not have repeated priors. \n",
        "\n",
        "This posterior, in theory, should be equivalent to the graphical model, giving the same accurate estimates of the\n",
        "Sersic indexes with precise errors. The process extracts the same information, fully accounting for the 2D structure \n",
        "of the PDF between the two Sersic indexes for each fit.\n",
        "\n",
        "However, in practise, this will likely not work that well. Every time we use a KDE to represent and multiply a \n",
        "posterior, we make an approximation which will impact our inferred errors. The removal of the prior before combining \n",
        "the likelihood and reinserting it after also introduces approximations, especially because the fit performed by the \n",
        "non-linear search is informed by the prior. \n",
        "\n",
        "Crucially, whilst posterior multiplication maybe sort-of-works-ok in two dimensions, for models with many more \n",
        "dimensions and degeneracies between parameters that are in 3D, 4D of more dimensions it simply does not work.\n",
        "\n",
        "In contrast, a graphical model fully samples all of the information a large dataset contains about the model, without\n",
        "making an approximations. In this sense, irrespective of how complex the model gets, it will fully extract the \n",
        "information contained in the dataset.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "In this tutorial, we demonstrated the strengths of a graphical model over fitting each dataset one-by-one. \n",
        "\n",
        "We argued that irrespective of how one may try to combine the results of many individual fits, the approximations that \n",
        "are made will always lead to a suboptimal estimation of the model parameters and fail to fully extract all information\n",
        "from the dataset. \n",
        "\n",
        "Furthermore, we argued that for high dimensional complex models a graphical model is the only way to fully extract\n",
        "all of the information contained in the dataset.\n",
        "\n",
        "In the next tutorial, we will consider a natural extension of a graphical model called a hierarchical model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}