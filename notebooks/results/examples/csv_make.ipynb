{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: CSV\n",
    "============\n",
    "\n",
    "In this tutorial, we use the aggregator to load the results of model-fits and output them in a single .csv file.\n",
    "\n",
    "This enables the results of many model-fits to be concisely summarised and inspected in a single table, which\n",
    "can also be easily passed on to other collaborators.\n",
    "\n",
    "__Interferometer__\n",
    "\n",
    "This script can easily be adapted to analyse the results of charge injection imaging model-fits.\n",
    "\n",
    "The only entries that needs changing are:\n",
    "\n",
    " - `ImagingAgg` -> `InterferometerAgg`.\n",
    " - `FitImagingAgg` -> `FitInterferometerAgg`.\n",
    " - `ImagingPlotter` -> `InterferometerPlotter`.\n",
    " - `FitImagingPlotter` -> `FitInterferometerPlotter`.\n",
    "\n",
    "Quantities specific to an interfometer, for example its uv-wavelengths real space mask, are accessed using the same API\n",
    "(e.g. `values(\"dataset.uv_wavelengths\")` and `.values{\"dataset.real_space_mask\")).\n",
    "\n",
    "__Database File__\n",
    "\n",
    "The aggregator can also load results from a `.sqlite` database file.\n",
    "\n",
    "This is beneficial when loading results for large numbers of model-fits (e.g. more than hundreds)\n",
    "because it is optimized for fast querying of results.\n",
    "\n",
    "See the package `results/database` for a full description of how to set up the database and the benefits it provides,\n",
    "especially if loading results from hard-disk is slow."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "from pyprojroot import here\n",
    "workspace_path = str(here())\n",
    "%cd $workspace_path\n",
    "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import autofit as af\n",
    "import autogalaxy as ag\n",
    "import autogalaxy.plot as aplt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Fit__\n",
    "\n",
    "The code below performs a model-fit using nautilus. \n",
    "\n",
    "You should be familiar with modeling already, if not read the `modeling/start_here.py` script before reading this one!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i in range(2):\n",
    "    dataset_name = f\"simple\"\n",
    "    dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
    "\n",
    "    dataset = ag.Imaging.from_fits(\n",
    "        data_path=path.join(dataset_path, \"data.fits\"),\n",
    "        psf_path=path.join(dataset_path, \"psf.fits\"),\n",
    "        noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
    "        pixel_scales=0.1,\n",
    "    )\n",
    "\n",
    "    mask = ag.Mask2D.circular(\n",
    "        shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
    "    )\n",
    "\n",
    "    dataset = dataset.apply_mask(mask=mask)\n",
    "\n",
    "    bulge = af.Model(ag.lp_linear.Sersic)\n",
    "    disk = af.Model(ag.lp_linear.Exponential)\n",
    "    bulge.centre = disk.centre\n",
    "\n",
    "    galaxy = af.Model(ag.Galaxy, redshift=0.5, bulge=bulge, disk=disk)\n",
    "\n",
    "    model = af.Collection(galaxies=af.Collection(galaxy=galaxy))\n",
    "\n",
    "    search = af.Nautilus(\n",
    "        path_prefix=path.join(\"results_folder_csv\"),\n",
    "        name=\"results\",\n",
    "        unique_tag=f\"simple_{i}\",\n",
    "        n_live=100,\n",
    "        number_of_cores=1,\n",
    "    )\n",
    "\n",
    "    class AnalysisLatent(ag.AnalysisImaging):\n",
    "        def compute_latent_variables(self, instance):\n",
    "            return {\"example_latent\": instance.galaxies.galaxy.bulge.sersic_index * 2.0}\n",
    "\n",
    "    analysis = AnalysisLatent(dataset=dataset)\n",
    "\n",
    "    result = search.fit(model=model, analysis=analysis)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Aggregator__\n",
    "\n",
    "Set up the aggregator as shown in `start_here.py`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from autofit.aggregator.aggregator import Aggregator\n",
    "\n",
    "agg = Aggregator.from_directory(\n",
    "    directory=path.join(\"output\", \"results_folder_csv\"),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Extract the `AggregateCSV` object, which has specific functions for outputting results in a CSV format."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agg_csv = af.AggregateCSV(aggregator=agg)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Adding CSV Columns_\n",
    "\n",
    "We first make a simple .csv which contains two columns, corresponding to the inferred median PDF values for\n",
    "the y centre of the bulge of the galaxy and its effective radius.\n",
    "\n",
    "To do this, we use the `add_column` method, which adds a column to the .csv file we write at the end. Every time\n",
    "we call `add_column` we add a new column to the .csv file.\n",
    "\n",
    "Note the API for the `centre`, which is a tuple parameter and therefore needs for `centre_0` to be specified.\n",
    "\n",
    "The `results_folder` contained three model-fits to three different datasets, meaning that each `add_column` call\n",
    "will add three rows, corresponding to the three model-fits.\n",
    "\n",
    "This adds the median PDF value of the parameter to the .csv file, we show how to add other values later in this script."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agg_csv.add_column(argument=\"galaxies.galaxy.bulge.centre.centre_0\")\n",
    "agg_csv.add_column(argument=\"galaxies.galaxy.bulge.sersic_index\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Saving the CSV__\n",
    "\n",
    "We can now output the results of all our model-fits to the .csv file, using the `save` method.\n",
    "\n",
    "This will output in your current working directory (e.g. the `autogalaxy_workspace`) as a .csv file containing the \n",
    "median PDF values of the parameters, have a quick look now to see the format of the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agg_csv.save(path=\"csv_simple.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Customizing CSV Headers__\n",
    "\n",
    "The headers of the .csv file are by default the argument input above based on the model. \n",
    "\n",
    "However, we can customize these headers using the `name` input of the `add_column` method, for example making them\n",
    "shorter or more readable.\n",
    "\n",
    "We recreate the `agg_csv` first, so that we begin adding columns to a new .csv file."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agg_csv = af.AggregateCSV(aggregator=agg)\n",
    "\n",
    "agg_csv.add_column(\n",
    "    argument=\"galaxies.galaxy.bulge.centre.centre_0\",\n",
    "    name=\"bulge_centre._0\",\n",
    ")\n",
    "agg_csv.add_column(\n",
    "    argument=\"galaxies.galaxy.bulge.sersic_index\",\n",
    "    name=\"bulge_sersic_index\",\n",
    ")\n",
    "\n",
    "agg_csv.save(path=\"csv_simple_custom_headers.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Maximum Likelihood Values__\n",
    "\n",
    "We can also output the maximum likelihood values of each parameter to the .csv file, using the `use_max_log_likelihood`\n",
    "input."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agg_csv = af.AggregateCSV(aggregator=agg)\n",
    "\n",
    "agg_csv.add_column(\n",
    "    argument=\"galaxies.galaxy.bulge.effective_radius\",\n",
    "    name=\"bulge_effective_radius_max_lh\",\n",
    "    use_max_log_likelihood=True,\n",
    ")\n",
    "\n",
    "agg_csv.save(path=\"csv_simple_max_likelihood.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Errors__\n",
    "\n",
    "We can also output PDF values at a given sigma confidence of each parameter to the .csv file, using \n",
    "the `use_values_at_sigma` input and specifying the sigma confidence.\n",
    "\n",
    "Below, we add the values at 3.0 sigma confidence to the .csv file, in order to compute the errors you would \n",
    "subtract the median value from these values.\n",
    "\n",
    "The method below adds two columns to the .csv file, corresponding to the values at the lower and upper sigma values."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# agg_csv = af.AggregateCSV(aggregator=agg)\n",
    "#\n",
    "# agg_csv.add_column(\n",
    "#     argument=\"galaxies.galaxy.bulge.effective_radius\",\n",
    "#     name=\"bulge_effective_radius_sigma\",\n",
    "#     use_values_at_sigma=3.0,\n",
    "# )\n",
    "#\n",
    "# agg_csv.save(path=\"csv_simple_errors.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Column List__\n",
    "\n",
    "We can add a list of values to the .csv file, provided the list is the same length as the number of model-fits\n",
    "in the aggregator.\n",
    "\n",
    "A useful example of this would be adding the name of every dataset to the .csv file in a column on the left,\n",
    "which would allow you to know which dataset each row corresponds to.\n",
    "\n",
    "To make this list, we use the `Aggregator` to loop over the `search` objects and extract their `unique_tag`'s, which \n",
    "when we fitted the model above used the dataset names. This API can also be used to extract the `name` or `path_prefix`\n",
    "of the search and build an informative list for the names of the subplots.\n",
    "\n",
    "We then pass this list to the `add_column` method, which will add a column to the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# unique_tag_list = [search.unique_tag for search in agg.values(\"search\")]\n",
    "\n",
    "# agg_csv = af.AggregateCSV(aggregator=agg)\n",
    "\n",
    "# agg_csv.add_column(\n",
    "#     argument=unique_tag_list,\n",
    "#     name=\"dataset_name\",\n",
    "# )\n",
    "\n",
    "# agg_csv.save(path=\"csv_simple_dataset_name.csv\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Latent Variables__\n",
    "\n",
    "Latent variables are not free model parameters but can be derived from the model, and they are described fully in\n",
    "?.\n",
    "\n",
    "This example was run with a latent variable called `example_latent`, and below we show that this latent variable\n",
    "can be added to the .csv file using the same API as above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# agg_csv = af.AggregateCSV(aggregator=agg)\n",
    "#\n",
    "# agg_csv.add_column(\n",
    "#     argument=\"example_latent\",\n",
    "# )\n",
    "#\n",
    "# agg_csv.save(path=\"csv_example_latent.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Computed Columns__\n",
    "\n",
    "We can also add columns to the .csv file that are computed from the median PDF instance values of the model.\n",
    "\n",
    "To do this, we write a function which is input into the `add_computed_column` method, where this function takes the\n",
    "median PDF instance as input and returns the computed value.\n",
    "\n",
    "Below, we add a trivial example of a computed column, where the value is twice the sersic index of the bulge."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def sersic_index_x2_from(samples):\n",
    "    instance = samples.median_pdf()\n",
    "\n",
    "    return 2.0 * instance.galaxies.galaxy.bulge.sersic_index\n",
    "\n",
    "\n",
    "agg_csv = af.AggregateCSV(aggregator=agg)\n",
    "\n",
    "agg_csv.add_computed_column(\n",
    "    name=\"bulge_sersic_index_x2_computed\",\n",
    "    compute=sersic_index_x2_from,\n",
    ")\n",
    "\n",
    "agg_csv.save(path=\"csv_computed_columns.csv\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
